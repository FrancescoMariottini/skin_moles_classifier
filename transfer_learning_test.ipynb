{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/rg+RRKsyHY8T+NaAVSj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silventesa/challenge-mole/blob/francesco/transfer_learning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz_h6gnZkDNE"
      },
      "source": [
        "# Transfer learning part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebp1FGMZ10yK"
      },
      "source": [
        "from keras.models import Model, Sequential\r\n",
        "from keras.layers import Dense,GlobalAveragePooling2D,Dropout,Flatten, Conv2D, MaxPooling2D\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "\r\n",
        "# lets initialize the VGG-16 model\r\n",
        "# We then remove the final layer of the model as we will add our own to only classify cats and dogs\r\n",
        "# We also decide the size of the input images: here they are 64px by 64px.\r\n",
        "\r\n",
        "prior_model = VGG16(weights='imagenet',include_top=False, input_shape=(64,64,3))\r\n",
        "\r\n",
        "# lets create our model\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# and here we add a all the VGG16 as a layer\r\n",
        "\r\n",
        "model.add(prior_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sDxhZujKI0-",
        "outputId": "34330921-7565-46a4-a990-4fb683a5199e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NGF6bt-TvJY",
        "outputId": "e8005234-d438-4710-bdd5-7277067143bd"
      },
      "source": [
        "!ls \"/content/drive/MyDrive/cats_dogs/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "single_prediction  test_set  training_set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U60tttOKZrW",
        "outputId": "970fa3bf-06c6-417e-813a-b422a9ffcc0c"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/training_set\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats  dogs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdqoWub-15kN",
        "outputId": "552839d5-1827-457f-a368-30547466a143"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGENT1h92CJz",
        "outputId": "63affe4b-6f98-4236-b4ce-f7c7c6ff1910"
      },
      "source": [
        "model.layers[0].summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af2XNioO2K8l"
      },
      "source": [
        "We see that VGG-16 is composed of Conv2D layers.\r\n",
        "and its final layer is a MaxPooling2D layer.\r\n",
        "\r\n",
        "In order to finish our model, we need to flatten it before providing it a Dense layer for the classification.\r\n",
        "\r\n",
        "You could add a couple of additional layers such as a Dropout or an other Dense layer before adding the softmax's one just like bellow.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ8bpPKW6gAc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbCYjvy2MNE"
      },
      "source": [
        "model.add(Flatten())\r\n",
        "model.add(Dense(256,activation='relu'))\r\n",
        "model.add(Dropout(0.1))\r\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ym21L_2PHd",
        "outputId": "04c3e41a-615c-4c82-fb73-49cf5f77259a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 15,239,746\n",
            "Trainable params: 15,239,746\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpfdURrX2XHj"
      },
      "source": [
        "for layers in model.layers[0].layers: # looping over each layers in layer 0 to freeze them\r\n",
        "  layers.trainable = False\r\n",
        "\r\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUU209zq2udt",
        "outputId": "55675c53-126c-4f97-ddb1-357695f4dd2c"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from datetime import datetime\r\n",
        "\r\n",
        "# compiling the model\r\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "# defining the constants for the model training\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "EPOCHS = 20\r\n",
        "URL_TRAINING = './training_set' \r\n",
        "URL_TESTING = './test_set' \r\n",
        "#google drive link\r\n",
        "URL_TRAINING = \"/content/drive/My Drive/Colab Notebooks/training_set\"\r\n",
        "URL_TESTING = \"/content/drive/My Drive/Colab Notebooks/test_set\"\r\n",
        "\r\n",
        "# creating the image generator\r\n",
        "\r\n",
        "generator = ImageDataGenerator(\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    rescale=1/255,\r\n",
        "    shear_range=0.2,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest',\r\n",
        "    validation_split=0.2\r\n",
        ")\r\n",
        "test_generator = ImageDataGenerator(\r\n",
        "    rescale=1/255,\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "# creating the train and test sets\r\n",
        "\r\n",
        "train_set = generator.flow_from_directory(URL_TRAINING, target_size=(64,64), batch_size=BATCH_SIZE)\r\n",
        "test_set = test_generator.flow_from_directory(URL_TESTING, target_size=(64,64), batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        " \r\n",
        "# fitting the model\r\n",
        "\r\n",
        "#model.fit_generator(train_set, steps_per_epoch=len(train_set.filenames)//BATCH_SIZE, epochs=EPOCHS, validation_data = test_set, validation_steps=len(test_set.filenames)//BATCH_SIZE )\r\n",
        "s = datetime.now()\r\n",
        "model.fit(train_set, steps_per_epoch=len(train_set.filenames)//BATCH_SIZE, epochs=EPOCHS, validation_data = test_set, validation_steps=len(test_set.filenames)//BATCH_SIZE )\r\n",
        "print(f'Fitting time: {str(datetime.now()-s)}')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1275 images belonging to 2 classes.\n",
            "Found 378 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "39/39 [==============================] - 286s 7s/step - loss: 0.7533 - accuracy: 0.5968 - val_loss: 0.5587 - val_accuracy: 0.7188\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.5346 - accuracy: 0.7183 - val_loss: 0.5828 - val_accuracy: 0.7159\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.5108 - accuracy: 0.7375 - val_loss: 0.6274 - val_accuracy: 0.7017\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 0.4818 - accuracy: 0.7752 - val_loss: 0.6468 - val_accuracy: 0.6733\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.4951 - accuracy: 0.7590 - val_loss: 0.5624 - val_accuracy: 0.7415\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.4887 - accuracy: 0.7624 - val_loss: 0.4771 - val_accuracy: 0.7500\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.4733 - accuracy: 0.7769 - val_loss: 0.5219 - val_accuracy: 0.7443\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 0.4433 - accuracy: 0.8073 - val_loss: 0.5874 - val_accuracy: 0.7301\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.4336 - accuracy: 0.7974 - val_loss: 0.5440 - val_accuracy: 0.7386\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 5s 122ms/step - loss: 0.4332 - accuracy: 0.8091 - val_loss: 0.5760 - val_accuracy: 0.7330\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.4251 - accuracy: 0.7936 - val_loss: 0.5040 - val_accuracy: 0.7415\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 0.4355 - accuracy: 0.8059 - val_loss: 0.5333 - val_accuracy: 0.7415\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 5s 124ms/step - loss: 0.4321 - accuracy: 0.7846 - val_loss: 0.4918 - val_accuracy: 0.7784\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.4552 - accuracy: 0.7913 - val_loss: 0.5392 - val_accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 0.4303 - accuracy: 0.7893 - val_loss: 0.5107 - val_accuracy: 0.7557\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 5s 126ms/step - loss: 0.4251 - accuracy: 0.8068 - val_loss: 0.4792 - val_accuracy: 0.7614\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.4255 - accuracy: 0.8064 - val_loss: 0.4985 - val_accuracy: 0.7812\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 5s 123ms/step - loss: 0.4259 - accuracy: 0.7904 - val_loss: 0.5445 - val_accuracy: 0.7330\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 5s 121ms/step - loss: 0.4113 - accuracy: 0.8293 - val_loss: 0.5310 - val_accuracy: 0.7472\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 5s 120ms/step - loss: 0.4527 - accuracy: 0.7764 - val_loss: 0.4987 - val_accuracy: 0.7472\n",
            "Fitting time: 0:06:22.107874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deAJZycPOij_",
        "outputId": "8dc255bd-64a6-4d21-ce6a-63cc829c5fa1"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.preprocessing import image\r\n",
        "\r\n",
        "TEST_IMAGE_URL = '/content/drive/My Drive/Colab Notebooks/test_image.jpg'\r\n",
        "\r\n",
        "test_image = image.load_img( TEST_IMAGE_URL , target_size = (64, 64))\r\n",
        "test_image = image.img_to_array(test_image)\r\n",
        "test_image = np.expand_dims(test_image, axis = 0)\r\n",
        "result = model.predict(test_image)\r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dIngJ2uPErh",
        "outputId": "74793427-07c0-43b4-fe4c-535fb9333efb"
      },
      "source": [
        "model.evaluate(test_set, steps=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/32 [==========>...................] - ETA: 1s - loss: 0.5153 - accuracy: 0.7407WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.5153 - accuracy: 0.7407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5152507424354553, 0.7407407164573669]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQp4X508SnFH"
      },
      "source": [
        "PATH = '/content/drive/My Drive/Colab Notebooks/64_by_64.h5'\r\n",
        "model.save(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx4F951ekO4j"
      },
      "source": [
        "# Transfer learning part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dj3V-CskkOd"
      },
      "source": [
        "We can see that our input layer is of shape 64,64,3.\r\n",
        "However, since we want to increase the details of the image taken for classification, we need to increase the number of pixels the input will take in. Lets double it this time. We will now have a input shape of 128, 128, 3.\r\n",
        "To do so, we will need to change the input layer of the VGG-16, as well as the first Conv2D layer.\r\n",
        "\r\n",
        "Why ?\r\n",
        "Because the first conv2D layer is mapped to the input layer, meaning that it has a shape of 64 by 64.\r\n",
        "As we are going to increase the number of imputed pixels to 128 by 128, our new conv2D will have to be of shape 128, 128 as well, while keeping the number of filter to 64.\r\n",
        "\r\n",
        "This means that while we froze the other layers of the VGG-16, we will have to re-train the first conv2D to learn on the new input shape, then pass forward the information to the deeper layers who will remain frozen.\r\n",
        "\r\n",
        "Why don't we have to change the other layers as well?  Because we will max pool our new conv2D layer of shape 128,128 into a shape 64, 64 which will match the shape of the second Conv2D layer in the VGG-16 model.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Now how are we going to proceed?\r\n",
        "\r\n",
        "We will first initiate a new model with two layers, one conv2D for input shape 128,128\r\n",
        "then a maxpooling layer which will divide the shape of the inputs by 2 (or more precisely, the shape of the filter image by 2) as 128 / 2 = 64.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0g8udf0kuUs"
      },
      "source": [
        "model2 = Sequential()\r\n",
        "model2.add(Conv2D(64,kernel_size=(3,3),input_shape=(128,128,3),activation='relu', padding='same'))\r\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJYSKwSIpqps"
      },
      "source": [
        "From here we just have to add the layers from the VGG-16 to our newly created model.\r\n",
        "How so ? Since we have already frozen the VGG-16 in our previous model, we just need to loop over it and add each layers to our brand new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtkfVpANpLxf"
      },
      "source": [
        "for layer in model.layers[0].layers[2:]:\r\n",
        "  # here we precise that we want to take all the layers from the second one to the last one in the VGG-16 model.\r\n",
        "  model2.add(layer)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTMKp5Q4pu8E"
      },
      "source": [
        "Finally we add all the other layers in our previous model, while freezing them in the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wqY2bawpawj"
      },
      "source": [
        "for layer in model.layers[1:]:\r\n",
        "  layer.trainable = False\r\n",
        "  model2.add(layer) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MngFWmb7qEUs"
      },
      "source": [
        "# compiling the model\r\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvGoXwimqtDt",
        "outputId": "6bae71a2-384a-49c7-c3c1-2ff37c606f48"
      },
      "source": [
        "# keeping the constants for the model training\r\n",
        "# keeping the same image generator\r\n",
        "# creating the train and test sets with different size\r\n",
        "train_set2 = generator.flow_from_directory(URL_TRAINING, target_size=(128,128), batch_size=BATCH_SIZE)\r\n",
        "test_set2 = test_generator.flow_from_directory(URL_TESTING, target_size=(128,128), batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1275 images belonging to 2 classes.\n",
            "Found 378 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-NQMJzGrHiM",
        "outputId": "7fdf81f3-c780-48d6-bf1d-edef6116bed0"
      },
      "source": [
        "# fitting the model\r\n",
        "s = datetime.now()\r\n",
        "model2.fit(train_set2, steps_per_epoch=len(train_set2.filenames)//BATCH_SIZE, epochs=EPOCHS, validation_data = test_set2, validation_steps=len(test_set2.filenames)//BATCH_SIZE )\r\n",
        "print(f'Fitting time: {str(datetime.now()-s)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 8s 212ms/step - loss: 0.6699 - accuracy: 0.6082 - val_loss: 0.6773 - val_accuracy: 0.5653\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.6679 - accuracy: 0.6026 - val_loss: 0.6675 - val_accuracy: 0.5824\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.6527 - accuracy: 0.6090 - val_loss: 0.6713 - val_accuracy: 0.5710\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.6519 - accuracy: 0.6163 - val_loss: 0.6464 - val_accuracy: 0.6193\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.6506 - accuracy: 0.6283 - val_loss: 0.6567 - val_accuracy: 0.5909\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6600 - accuracy: 0.6098 - val_loss: 0.6621 - val_accuracy: 0.5824\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6537 - accuracy: 0.6090 - val_loss: 0.6670 - val_accuracy: 0.5881\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6399 - accuracy: 0.6323 - val_loss: 0.6703 - val_accuracy: 0.5795\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 8s 208ms/step - loss: 0.6507 - accuracy: 0.6195 - val_loss: 0.6783 - val_accuracy: 0.5710\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6558 - accuracy: 0.6227 - val_loss: 0.6867 - val_accuracy: 0.5682\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.6579 - accuracy: 0.6106 - val_loss: 0.6510 - val_accuracy: 0.6250\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.6492 - accuracy: 0.6114 - val_loss: 0.6572 - val_accuracy: 0.5909\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6450 - accuracy: 0.6203 - val_loss: 0.6905 - val_accuracy: 0.5483\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 8s 209ms/step - loss: 0.6453 - accuracy: 0.6283 - val_loss: 0.6590 - val_accuracy: 0.5824\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.6336 - accuracy: 0.6356 - val_loss: 0.6611 - val_accuracy: 0.5938\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.6447 - accuracy: 0.6267 - val_loss: 0.6582 - val_accuracy: 0.5909\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 8s 211ms/step - loss: 0.6427 - accuracy: 0.6275 - val_loss: 0.6539 - val_accuracy: 0.5994\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.6502 - accuracy: 0.6098 - val_loss: 0.6492 - val_accuracy: 0.5824\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 8s 210ms/step - loss: 0.6430 - accuracy: 0.6259 - val_loss: 0.6391 - val_accuracy: 0.6051\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 8s 207ms/step - loss: 0.6347 - accuracy: 0.6340 - val_loss: 0.6345 - val_accuracy: 0.6136\n",
            "Fitting time: 0:02:43.957438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMAqAuQOrGr4",
        "outputId": "96cba319-d3b5-49bc-c6ea-7186a77f21b5"
      },
      "source": [
        "model2.evaluate(test_set2, steps=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/32 [==========>...................] - ETA: 1s - loss: 0.6391 - accuracy: 0.6085WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.6391 - accuracy: 0.6085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6391320824623108, 0.6084656119346619]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOynOz_ZvX1t",
        "outputId": "76c7c185-c560-4a85-c4c2-3a675b42dc50"
      },
      "source": [
        "TEST_IMAGE_URL = '/content/drive/My Drive/Colab Notebooks/test_image.jpg'\r\n",
        "\r\n",
        "test_image = image.load_img( TEST_IMAGE_URL , target_size = (128, 128))\r\n",
        "test_image = image.img_to_array(test_image)\r\n",
        "test_image = np.expand_dims(test_image, axis = 0)\r\n",
        "result2 = model2.predict(test_image)\r\n",
        "print(result2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.2735995e-21 1.0000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsQa55ev-CV"
      },
      "source": [
        "TEST_IMAGE_URL2 = '/content/drive/My Drive/Colab Notebooks/test_image2.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG06uSAOv_gD",
        "outputId": "ab516a86-3be1-47a3-bcda-e53a6ff66656"
      },
      "source": [
        "test_image2 = image.load_img( TEST_IMAGE_URL2 , target_size = (128, 128))\r\n",
        "test_image2 = image.img_to_array(test_image2)\r\n",
        "test_image2 = np.expand_dims(test_image2, axis = 0)\r\n",
        "result22 = model2.predict(test_image2)\r\n",
        "print(result22)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.2157044e-09 1.0000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}